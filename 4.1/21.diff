commit a0a9d45fa79062034a7c6fc7423accb89a6df60f
Author: Ilya V. Matveychikov <matvejchikov@gmail.com>
Date:   Sun Jun 5 17:25:00 2016 +0300

    skbuff: prevent expanding tail of splitted SKB (#4)
    
    When modifying SKB's data, Tempesta may map a piece of skb's linear data to
    a paged fragment. Once that kind of data mapping is done, any subsequent
    skb modification cannot use the fast path optimization where skb->tail is
    moved forward towards skb->end.
    
    This adds skb->tail_lock flag and ensures that skb_is_nonlinear() takes
    it's value into account. The latter means that all the SKBs which has
    tail_lock flagged acts as NON-linear and skb_tailroom() returns 0.
    
    The kernel has several places where SKB tailroom is being calculated
    without using skb_tailroom() (skb_pad, __pskb_pull_tail). We need to
    fix all those calls to be familiar with tail locking.
    
    For more information see the issue:
    https://github.com/tempesta-tech/tempesta/issues/503

diff --git a/include/linux/skbuff.h b/include/linux/skbuff.h
index f915c872..efbefac7 100644
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@ -622,6 +622,9 @@ struct sk_buff {
 	__u8			inner_protocol_type:1;
 	__u8			remcsum_offload:1;
 	/* 3 or 5 bit hole */
+#ifdef CONFIG_SECURITY_TEMPESTA
+	__u8			tail_lock:1;
+#endif
 
 #ifdef CONFIG_NET_SCHED
 	__u16			tc_index;	/* traffic control index */
@@ -1560,7 +1563,7 @@ static inline struct sk_buff *__skb_dequeue_tail(struct sk_buff_head *list)
 
 static inline bool skb_is_nonlinear(const struct sk_buff *skb)
 {
-	return skb->data_len;
+	return skb->tail_lock || skb->data_len;
 }
 
 static inline unsigned int skb_headlen(const struct sk_buff *skb)
@@ -1746,6 +1749,18 @@ static inline unsigned int skb_headroom(const struct sk_buff *skb)
 	return skb->data - skb->head;
 }
 
+/**
+ *	skb_tailroom_locked - bytes at buffer end
+ *	@skb: buffer to check
+ *
+ *	Return the number of bytes of free space at the tail of an sk_buff with
+ *	respect to tail locking only.
+ */
+static inline int skb_tailroom_locked(const struct sk_buff *skb)
+{
+	return skb->tail_lock ? 0 : skb->end - skb->tail;
+}
+
 /**
  *	skb_tailroom - bytes at buffer end
  *	@skb: buffer to check
diff --git a/net/core/skbuff.c b/net/core/skbuff.c
index 910bf009..c1228b8c 100644
--- a/net/core/skbuff.c
+++ b/net/core/skbuff.c
@@ -1420,6 +1420,7 @@ int pskb_expand_head(struct sk_buff *skb, int nhead, int ntail,
 	skb->head     = data;
 #ifdef CONFIG_SECURITY_TEMPESTA
 	skb->head_frag = 1;
+	skb->tail_lock = 0;
 #else
 	skb->head_frag = 0;
 #endif
@@ -1552,7 +1553,7 @@ int skb_pad(struct sk_buff *skb, int pad)
 		return 0;
 	}
 
-	ntail = skb->data_len + pad - (skb->end - skb->tail);
+	ntail = skb->data_len + pad - skb_tailroom_locked(skb);
 	if (likely(skb_cloned(skb) || ntail > 0)) {
 		err = pskb_expand_head(skb, 0, ntail, GFP_ATOMIC);
 		if (unlikely(err))
@@ -1787,7 +1788,7 @@ unsigned char *__pskb_pull_tail(struct sk_buff *skb, int delta)
 	 * plus 128 bytes for future expansions. If we have enough
 	 * room at tail, reallocate without expansion only if skb is cloned.
 	 */
-	int i, k, eat = (skb->tail + delta) - skb->end;
+	int i, k, eat = delta - skb_tailroom_locked(skb);
 
 	if (eat > 0 || skb_cloned(skb)) {
 		if (pskb_expand_head(skb, 0, eat > 0 ? eat + 128 : 0,
